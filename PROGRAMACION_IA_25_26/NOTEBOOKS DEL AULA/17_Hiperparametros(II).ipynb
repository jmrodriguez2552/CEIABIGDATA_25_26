{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "7f0fbaec",
            "metadata": {},
            "source": [
                "\n",
                "# HIPERPAR√ÅMETROS (II)\n",
                "## Aprendizaje Supervisado: Modelos, Hiperpar√°metros y Ajuste\n",
                "\n",
                "Este notebook presenta una explicaci√≥n te√≥rica detallada de los modelos de aprendizaje supervisado m√°s comunes, junto con sus principales hiperpar√°metros y recomendaciones para su ajuste. Adem√°s, se incluyen ejemplos pr√°cticos en Python utilizando `GridSearchCV` y `RandomizedSearchCV`."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f2895292",
            "metadata": {},
            "source": [
                "\n",
                "## 1. Modelos y Hiperpar√°metros\n",
                "\n",
                "### üîπ Regresi√≥n Lineal / Log√≠stica\n",
                "\n",
                "#### Hiperpar√°metros principales   \n",
                "- `penalty`: tipo de regularizaci√≥n (l1, l2, elasticnet, none)\n",
                "- `C`: inverso de la regularizaci√≥n. Cuanto m√°s peque√±o, m√°s regularizaci√≥n.\n",
                "- `solver`: algoritmo de optimizaci√≥n. (Por defecto `lbfgs` para regresi√≥n log√≠stica y `saga` para regresi√≥n lineal)\n",
                "- `max_iter`: iteraciones m√°ximas para convergencia.\n",
                "\n",
                "#### Ajustes\n",
                "- Usa `C ` para evitar sobreajuste (valores bajos de `C`) o bajoajuste (valores altos).\n",
                "- Prueba diferentes `penalty` seg√∫n el tipo de sparsity deseado: `l1` para modelos m√°s simples, `l2` para estabilidad.\n",
                "- Aumenta `max_iter` si el modelo no converge.   \n",
                "---\n",
                "\n",
                "### üîπ √Årboles de Decisi√≥n   \n",
                "\n",
                "#### Hiperpar√°metros principales\n",
                "- `max_depth`: profundidad m√°xima del √°rbol\n",
                "- `min_samples_split`: m√≠nimo n√∫mero de muestras para dividir nodo\n",
                "- `min_samples_leaf`: m√≠nimo n√∫mero de muestras en una hoja\n",
                "- `max_features`: n¬∫ m√°ximo de caracter√≠sticas a considerar al dividir nodo\n",
                "- `criterion`: funci√≥n de evaluaci√≥n de la calidad de la divisi√≥n (gini, entropy)\n",
                "\n",
                "#### Ajustes\n",
                "- Aumenta `max_depth` para permitir √°rboles m√°s profundos o limita para evitar sobreajuste.\n",
                "- Prueba diferentes `min_samples_split` y `min_samples_leaf` seg√∫n el equilibrio entre complejidad y generalizaci√≥n. Aumenta `min_samples_split` para evitar sobreajuste y `min_samples_leaf` para evitar bajoajuste, de forma que `min_samples_split` sea mayor que `min_samples_leaf` y conseguir generalizaci√≥n mejorada.\n",
                "- Aumenta `max_features` para permitir m√°s caracter√≠sticas en cada divisi√≥n.\n",
                "- Usa `entropy` para √°rboles m√°s equilibrados. `gini` es m√°s r√°pido pero puede ser menos preciso y viene por defecto.\n",
                "---\n",
                "\n",
                "### üîπ Random Forest\n",
                "\n",
                "#### Hiperpar√°metros principales\n",
                "- `n_estimators`: n√∫mero de √°rboles\n",
                "- `max_depth`, `min_samples_split`, `min_samples_leaf`\n",
                "- `max_features`: n¬∫ m√°ximo de caracter√≠sticas a considerar al dividir nodo.\n",
                "- `bootstrap`: si se usa bootstrap para entrenar los √°rboles (por defecto `True`). Este par√°metro es importante para evitar sobreajuste y lo que hace es que cada √°rbol se entrena con una muestra aleatoria de los datos.\n",
                "\n",
                "#### Ajustes\n",
                "- Aumenta `n_estimators` para mejorar la generalizaci√≥n.\n",
                "- Aumenta `max_depth` para permitir √°rboles m√°s profundos o limita para evitar sobreajuste.\n",
                "- Prueba diferentes `min_samples_split` y `min_samples_leaf` seg√∫n el equilibrio entre complejidad y generalizaci√≥n. Aumenta `min_samples_split` para evitar sobreajuste y `min_samples_leaf` para evitar bajoajuste, de forma que `min_samples_split` sea mayor que `min_samples_leaf` y conseguir generalizaci√≥n mejorada.\n",
                "- Aumenta `max_features` para permitir m√°s caracter√≠sticas en cada divisi√≥n.\n",
                "- Usa `entropy` para √°rboles m√°s equilibrados. `gini` es m√°s r√°pido pero puede ser menos preciso y viene por defecto.\n",
                "---\n",
                "\n",
                "### üîπ Gradient Boosting (XGBoost, LightGBM, etc.)   \n",
                "\n",
                "#### Hiperpar√°metros principales\n",
                "- `n_estimators`: n√∫mero de √°rboles que se van a entrenar.\n",
                "- `learning_rate`: tasa de aprendizaje. A menor tasa de aprendizaje, mejor generalizaci√≥n.\n",
                "- `max_depth`: profundidad m√°xima del √°rbol.\n",
                "- `subsample`: proporci√≥n de muestras a considerar al dividir nodo.\n",
                "- `colsample_bytree`: proporci√≥n de caracter√≠sticas a considerar al dividir nodo.\n",
                "- `reg_alpha`, `reg_lambda`: par√°metros de regularizaci√≥n. `reg_alpha` es la regularizaci√≥n L1 y `reg_lambda` es la regularizaci√≥n L2.\n",
                "\n",
                "#### Ajustes\n",
                "- Disminuir `learning_rate` y aumentar `n_estimators` suele mejorar la generalizaci√≥n.\n",
                "- Usar `subsample` y `colsample_bytree` < 1 para reducir sobreajuste.\n",
                "- Ajustar `reg_alpha` y `reg_lambda` para evitar modelos demasiado complejos.\n",
                "--- \n",
                "\n",
                "### üîπ k-Nearest Neighbors    \n",
                "\n",
                "#### Hiperpar√°metros principales    \n",
                "- `n_neighbors`: n√∫mero de vecinos m√°s cercanos para la predicci√≥n.\n",
                "- `weights`: peso de los vecinos. Puede ser 'uniform' o 'distance'.\n",
                "- `metric`: m√©trica de distancia. Puede ser 'euclidean', 'manhattan', 'minkowski', 'chebyshev', 'wminkowski', 'seuclidean', 'mahalanobis' o 'cosine'.\n",
                "\n",
                "#### Ajustes\n",
                "- Aumenta `n_neighbors` para mejorar la generalizaci√≥n, aunque puede aumentar el sesgo.\n",
                "- Usa `weights` = 'distance' para dar m√°s peso a las muestras m√°s cercanas, lo que puede mejorar la generalizaci√≥n, sobretodo si el dataset es desequilibrado y tiene mucho ruido.\n",
                "- Prueba diferentes m√©tricas seg√∫n el tipo de datos y el problema. Tambi√©n prueba diferentes m√©tricas si las variables son de diferente escala. Por defecto viene 'euclidean'.\n",
                "---\n",
                "\n",
                "### üîπ Support Vector Machines   \n",
                "\n",
                "#### Hiperpar√°metros principales\n",
                "- `C`: penalizaci√≥n de la regularizaci√≥n (errores de clasificaci√≥n). Cuanto m√°s peque√±o, m√°s regularizaci√≥n.\n",
                "- `kernel`: kernel a usar. Puede ser 'linear', 'poly', 'rbf', 'sigmoid' o 'precomputed'.\n",
                "- `gamma`: controla el alcance de la influencia de un ejemplo (s√≥lo para kernels no lineales).\n",
                "\n",
                "#### Ajustes\n",
                "- Aumenta `C` para un ajuste m√°s preciso, aunque puede aumentar el sesgo. Reducir `C` puede mejorar la generalizaci√≥n y reducir el sobreajuste.\n",
                "- Usa kernel='linear' para alta dimensionalidad. Por defecto viene 'rbf', idoneo para problemas de baja dimensionalidad y relaciones no lineales.\n",
                "- `gamma` bajo ‚Üí m√°s general, `gamma` alto ‚Üí riesgo de sobreajuste.\n",
                "---\n",
                "\n",
                "### üîπ Redes Neuronales (MLP)\n",
                "\n",
                "#### Hiperpar√°metros principales    \n",
                "- `hidden_layer_sizes`: n√∫mero de neuronas en cada capa oculta.\n",
                "- `activation`: funci√≥n de activaci√≥n. Puede ser 'identity', 'logistic', 'tanh', 'relu'.\n",
                "- `solver`: algoritmo de optimizaci√≥n. Puede ser 'lbfgs', 'sgd', 'adam'.\n",
                "- `alpha`: regularizaci√≥n L2.\n",
                "- `learning_rate`: tasa de aprendizaje.\n",
                "- `max_iter`: iteraciones m√°ximas para convergencia. √âpocas.de entrenamiento.\n",
                "\n",
                "#### Ajustes\n",
                "- Usa pocas capas ocultas y pocas neuronas por capa para problemas de baja dimensionalidad, m√°s capas y m√°s neuronas para problemas de alta dimensionalidad.\n",
                "- Aumenta `hidden_layer_sizes` para mejorar la generalizaci√≥n, aunque puede aumentar el sesgo.\n",
                "- Usa `activation` = 'relu' para problemas de alta dimensionalidad. Por defecto viene 'relu', idoneo para problemas de baja dimensionalidad y relaciones no lineales.\n",
                "- `alpha` bajo ‚Üí m√°s general, `alpha` alto ‚Üí riesgo de sobreajuste. Aumenta `alpha` para mejorar la generalizaci√≥n, aunque puede aumentar el sesgo.\n",
                "- Usa validaci√≥n para ajustar `max_iter` y `learning_rate`.\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d3b93e07",
            "metadata": {},
            "source": [
                "## 2. M√©todos de Ajuste\n",
                "\n",
                "- **Grid Search**: b√∫squeda exhaustiva y costosa computacionalmente.\n",
                "- **Randomized Search**: b√∫squeda aleatoria. M√°s eficiente computacionalmente para muchas variables.\n",
                "- **Optuna / Hyperopt**: optimizaci√≥n bayesiana. Selecci√≥n inteligente de hiperpar√°metros, calculando la pr√≥xima iteraci√≥n basada en la informaci√≥n de las anteriores.\n",
                "- **Validaci√≥n cruzada**: para evitar sobreajuste y fundamental para evaluar el impacto real de los hiperpar√°metros. Para clasificaci√≥n se suele usar `StratifiedKFold`.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "25048503",
            "metadata": {},
            "source": [
                "## 3. Ejemplo pr√°ctico"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "3c68d925",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# Ejemplo pr√°ctico: Ajuste de hiperpar√°metros con GridSearchCV y RandomizedSearchCV\n",
                "\n",
                "from sklearn.datasets import load_breast_cancer\n",
                "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
                "from sklearn.metrics import classification_report\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.neural_network import MLPClassifier\n",
                "import numpy as np"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "35d1b942",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cargar datos\n",
                "X, y = load_breast_cancer(return_X_y=True)\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "b7d272b0",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Lista de modelos y sus grids\n",
                "models = {\n",
                "    \"LogisticRegression\": (LogisticRegression(max_iter=1000), {\n",
                "        'classifier__C': [0.01, 0.1, 1, 10], # Los valores que se ubicar√°n en el grid. Se seleccionan seg√∫n el conocimiento del problema y la experiencia previa.\n",
                "        'classifier__penalty': ['l2'], # 'l1' no es compatible con 'lbfgs'\n",
                "        'classifier__solver': ['lbfgs'] # 'liblinear' es compatible con 'l1' y 'l2', pero 'lbfgs' es m√°s eficiente para grandes conjuntos de datos\n",
                "    }),\n",
                "    \"DecisionTree\": (DecisionTreeClassifier(), {\n",
                "        'classifier__max_depth': [3, 5, 10, None],\n",
                "        'classifier__min_samples_split': [2, 5, 10] # N√∫mero m√≠nimo de muestras requeridas para dividir un nodo. Los valores indicados son comunes para evitar el sobreajuste.\n",
                "    }),\n",
                "    \"RandomForest\": (RandomForestClassifier(), {\n",
                "        'classifier__n_estimators': [50, 100], # N√∫mero de √°rboles en el bosque. Valores comunes son 50, 100, 200.\n",
                "        'classifier__max_depth': [None, 10, 20], # Profundidad m√°xima del √°rbol. Valores comunes son None (sin l√≠mite), 10, 20.\n",
                "        'classifier__max_features': ['sqrt'] # N√∫mero de caracter√≠sticas a considerar al buscar la mejor divisi√≥n. 'sqrt' es una opci√≥n com√∫n.\n",
                "    }),\n",
                "    \"GradientBoosting\": (GradientBoostingClassifier(), {\n",
                "        'classifier__n_estimators': [50, 100], # N√∫mero de etapas de refuerzo. Valores comunes son 50, 100.\n",
                "        'classifier__learning_rate': [0.01, 0.1], # Tasa de aprendizaje. Valores comunes son 0.01, 0.1.\n",
                "        'classifier__max_depth': [3, 5] # Profundidad m√°xima de los √°rboles individuales. Valores comunes son 3, 5.\n",
                "    }),\n",
                "    \"KNN\": (KNeighborsClassifier(), {\n",
                "        'classifier__n_neighbors': [3, 5, 7], # N√∫mero de vecinos a considerar. Valores comunes son 3, 5, 7.\n",
                "        'classifier__weights': ['uniform', 'distance'], # Estrategia de ponderaci√≥n. 'uniform' asigna el mismo peso a todos los vecinos, 'distance' pondera por la distancia.\n",
                "        'classifier__metric': ['euclidean', 'manhattan'] # M√©trica de distancia. 'euclidean' y 'manhattan' son opciones comunes.\n",
                "    }),\n",
                "    \"SVC\": (SVC(), {\n",
                "        'classifier__C': [0.1, 1, 10], # Par√°metro de regularizaci√≥n. Valores comunes son 0.1, 1, 10.\n",
                "        'classifier__kernel': ['linear', 'rbf'], # Tipo de kernel. 'linear' y 'rbf' son opciones comunes.\n",
                "        'classifier__gamma': ['scale', 'auto'] # Coeficiente del kernel. 'scale' y 'auto' son opciones comunes.\n",
                "    }),\n",
                "    \"MLP\": (MLPClassifier(max_iter=1000), {\n",
                "        'classifier__hidden_layer_sizes': [(50,), (100,)], # Tama√±o de las capas ocultas. Valores comunes son (50,), (100,).\n",
                "        'classifier__activation': ['relu', 'tanh'], # Funci√≥n de activaci√≥n. 'relu' y 'tanh' son opciones comunes.\n",
                "        'classifier__alpha': [0.0001, 0.001], # Par√°metro de regularizaci√≥n. Valores comunes son 0.0001, 0.001.\n",
                "        'classifier__learning_rate': ['constant', 'adaptive'] # Estrategia de tasa de aprendizaje. 'constant' y 'adaptive' son opciones comunes.\n",
                "    })\n",
                "}"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7582cf19",
            "metadata": {},
            "source": [
                "A continuaci√≥n se pone en marcha el `GridSearchCV`. Se construye un `Pipeline` en el que se encadena la secuencia de pasos del **preprocesado** (escalado en nuestro caso *scaler*) y el **modelo**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "33c9148a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Modelo: LogisticRegression\n",
                        "Mejores par√°metros (GridSearchCV): {'classifier__C': 0.1, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs'}\n",
                        "Accuracy en test: 0.9824561403508771\n",
                        "\n",
                        "Modelo: DecisionTree\n",
                        "Mejores par√°metros (GridSearchCV): {'classifier__max_depth': None, 'classifier__min_samples_split': 5}\n",
                        "Accuracy en test: 0.9298245614035088\n",
                        "\n",
                        "Modelo: RandomForest\n",
                        "Mejores par√°metros (GridSearchCV): {'classifier__max_depth': None, 'classifier__max_features': 'sqrt', 'classifier__n_estimators': 50}\n",
                        "Accuracy en test: 0.9649122807017544\n",
                        "\n",
                        "Modelo: GradientBoosting\n",
                        "Mejores par√°metros (GridSearchCV): {'classifier__learning_rate': 0.1, 'classifier__max_depth': 3, 'classifier__n_estimators': 100}\n",
                        "Accuracy en test: 0.956140350877193\n",
                        "\n",
                        "Modelo: KNN\n",
                        "Mejores par√°metros (GridSearchCV): {'classifier__metric': 'manhattan', 'classifier__n_neighbors': 3, 'classifier__weights': 'uniform'}\n",
                        "Accuracy en test: 0.9649122807017544\n",
                        "\n",
                        "Modelo: SVC\n",
                        "Mejores par√°metros (GridSearchCV): {'classifier__C': 0.1, 'classifier__gamma': 'scale', 'classifier__kernel': 'linear'}\n",
                        "Accuracy en test: 0.9824561403508771\n",
                        "\n",
                        "Modelo: MLP\n",
                        "Mejores par√°metros (GridSearchCV): {'classifier__activation': 'relu', 'classifier__alpha': 0.0001, 'classifier__hidden_layer_sizes': (50,), 'classifier__learning_rate': 'constant'}\n",
                        "Accuracy en test: 0.9736842105263158\n"
                    ]
                }
            ],
            "source": [
                "from sklearn.compose import ColumnTransformer\n",
                "# Aplicaci√≥n de GridSearchCV\n",
                "for name, (model, params) in models.items():\n",
                "    print(f\"\\nModelo: {name}\")\n",
                "    preprocess = ColumnTransformer(\n",
                "        transformers=[\n",
                "            ('scaler', StandardScaler(), slice(0, X.shape[1])) # Escalar todas las caracter√≠sticas\n",
                "        ]\n",
                "    )\n",
                "    pipe = Pipeline([                   # Crear un pipeline que incluya el escalador y el modelo.\n",
                "        ('prep', preprocess),\n",
                "        ('classifier', model)\n",
                "    ])\n",
                "    grid = GridSearchCV(pipe, params, cv=3, scoring='accuracy', n_jobs=-1) # pipe es el pipeline que incluye el escalador y el modelo\n",
                "    grid.fit(X_train, y_train)\n",
                "    print(\"Mejores par√°metros (GridSearchCV):\", grid.best_params_)\n",
                "    print(\"Accuracy en test:\", grid.score(X_test, y_test))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "c2e3c9ac",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Ejemplo con RandomizedSearchCV (RandomForest)\n",
                        "Mejores par√°metros (RandomizedSearchCV): {'classifier__max_depth': 10, 'classifier__min_samples_split': 3, 'classifier__n_estimators': 197}\n",
                        "Accuracy en test: 0.9649122807017544\n"
                    ]
                }
            ],
            "source": [
                "# Ejemplo con RandomizedSearchCV (s√≥lo uno para simplificar)\n",
                "print(\"\\nEjemplo con RandomizedSearchCV (RandomForest)\")\n",
                "from scipy.stats import randint\n",
                "rf = RandomForestClassifier()\n",
                "param_dist = {\n",
                "    'classifier__n_estimators': randint(10, 200),\n",
                "    'classifier__max_depth': [None, 10, 20, 30],\n",
                "    'classifier__min_samples_split': randint(2, 11),\n",
                "}\n",
                "pipe = Pipeline([\n",
                "    ('scaler', StandardScaler()),\n",
                "    ('classifier', rf)\n",
                "])\n",
                "random_search = RandomizedSearchCV(pipe, param_dist, n_iter=10, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\n",
                "random_search.fit(X_train, y_train)\n",
                "print(\"Mejores par√°metros (RandomizedSearchCV):\", random_search.best_params_)\n",
                "print(\"Accuracy en test:\", random_search.score(X_test, y_test))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
