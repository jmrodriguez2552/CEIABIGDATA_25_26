from openai import OpenAI
import sys

# 1. Configuraci√≥n del Cliente
# Apuntamos al servidor local de LM Studio
try:
    client = OpenAI(
        base_url="http://localhost:1234/v1", 
        api_key="lm-studio"
    )
except Exception as e:
    print(f"Error al configurar: {e}")
    sys.exit()

# 2. Datos de entrada (Las rese√±as del ejercicio)
lista_resenas = [
    "El producto lleg√≥ roto, terrible servicio.",
    "Me encant√≥, es justo lo que buscaba.",
    "El env√≠o fue r√°pido pero la calidad es regular."
]

# 3. Definici√≥n del System Prompt
# Es CRUCIAL ser muy estricto aqu√≠ para que no suelte un discurso, solo la etiqueta.
system_prompt = (
    "La clasificaci√≥n solo puede ser: 'Positivo', 'Negativo' o 'Neutro'."
)

print("--- ü§ñ INICIANDO CLASIFICACI√ìN DE RESE√ëAS ---")
print(f"Procesando {len(lista_resenas)} opiniones...\n")

# 4. Bucle de procesamiento
for rese√±a in lista_resenas:
    try:
        response = client.chat.completions.create(
            model="local-model", # En LM Studio usa el modelo que tengas cargado
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": rese√±a}
            ],
            temperature=0.1, # Temperatura baja = Mayor precisi√≥n y menos "creatividad"
        )

        # Extraemos la respuesta limpia
        clasificacion = response.choices[0].message.content.strip()

        # 5. Imprimir resultados formateados
        print(f"üìù Rese√±a: \"{rese√±a}\"")
        print(f"üè∑Ô∏è  Clasificaci√≥n: {clasificacion}")
        print("-" * 40)

    except Exception as e:
        print(f"‚ùå Error procesando la rese√±a: {e}")
        print("Aseg√∫rate de que el servidor de LM Studio est√° iniciado (Start Server).")
        break

print("\n‚úÖ Proceso terminado.")