{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCCIÓN AL NLP (NATURAL LANGUAGE PROCESSING - PROCESAMIENTO DE LENGUAJE NATURAL)\n",
    "\n",
    "## Conceptos básicos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOKENIZACIÓN\n",
    "\n",
    "En un proceso de NLP una de las primeras tareas a realizar va a ser eliminar las palabras que tienen poco interés para nuestro análisis.   \n",
    "El primer paso es delimitar las palabras del texto, y convertir esas palabras en elementos de una lista. Este procedimiento es conocido como **tokenización**. \n",
    "\n",
    "Procedemos a implementar esto en Python: (Usaremos la librería ***SPACY***-> **pip install spacy**)\n",
    "\n",
    "Nota:    \n",
    "Ejecutar en terminal: \n",
    "     \n",
    "***python -m spacy download es***   \n",
    "\n",
    "para descargar el modelo de idioma español-castellano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A instalar desde la terminal:\n",
    "# pip install spacy\n",
    "# python -m spacy download es_core_news_sm\n",
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "text = \"\"\"Soy un texto. Normalmente soy más largo y más grande. Que no te engañe mi tamaño.\"\"\"\n",
    "doc = nlp(text) # Crea un objeto de spacy tipo nlp\n",
    "tokens = [t.orth_ for t in doc] # Crea una lista con las palabras del texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto, ya tenemos nuestro texto convertido en una lista de **tokens**.    \n",
    "\n",
    "Lamentablemente, están todas las palabras. Se puede comprobar que también se incluyen como **tokens** los signos de puntuación.    \n",
    "\n",
    "Por nuestra parte, lo que nos interesa es quedarnos solo con aquellas que sean más o menos representativas del texto. Para ello, vamos a eliminar de esa lista las palabras muy comunes o poco informativas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, vamos a pedirle a esta librería que construya la lista de tokens pero que no incluya palabras muy comunes y poco informativas desde el punto de vista léxico, tales como conjunciones (y, o, ni, que), preposiciones (a, en, para, por, entre otras) y verbos muy comunes (ser, ir, y otros más).    \n",
    "\n",
    "Para realizarlo, se va a utilizar una condición que diga “todos los tokens del texto, siempre y cuando no se incluyan la puntuación ni las palabras poco representativas (stopwords)”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "text = \"\"\"Soy un texto. Normalmente soy más largo y más grande. Que no te engañe mi tamaño.\"\"\"\n",
    "doc = nlp(text)\n",
    "lexical_tokens = [t.orth_ for t in doc if not t.is_punct | t.is_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora ya disponemos de una lista de palabra que nos pueden dar una idea sobre la temática tratada en el texto o bien, que nos permita una clasificación más certera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NORMALIZACIÓN\n",
    "\n",
    "El siguiente paso en este flujo de trabajo consiste en **normalizar el texto**.      \n",
    "\n",
    "La **normalización de un texto** en el ámbito del procesamiento de lenguaje natural (NLP) es el proceso mediante el cual se transforma y estandariza el contenido textual para que los algoritmos y sistemas informáticos puedan procesarlo, analizarlo y entenderlo de manera eficaz y coherente.\n",
    "\n",
    "\n",
    "La normalización incluye varias tareas que garantizan que el texto se encuentre en un formato homogéneo. Los pasos más habituales son:\n",
    "\n",
    "- *Conversión a minúsculas*: Para evitar diferencias entre palabras como \"Casa\" y \"casa\", todo el texto se transforma a minúsculas.​\n",
    "- *Eliminación de caracteres especiales y signos de puntuación*: Se retiran aquellos símbolos que no aportan significado relevante, como “?” o “#”, para simplificar el análisis.​\n",
    "- *Eliminación de números*: Según el caso, los números pueden retirarse si no añaden valor al análisis del texto.​\n",
    "- *Eliminación de palabras vacías (\"stop words\")*: Son palabras muy comunes (como “de”, “en”, “a”) que, por lo general, no aportan información significativa para el análisis semántico y suelen eliminarse.​\n",
    "- *Tokenización*: Es el proceso de dividir el texto en unidades mínimas llamadas tokens (palabras, frases, símbolos).​\n",
    "- *Limpieza de formatos*: Eliminar saltos de línea, tabulaciones y otros elementos que puedan dificultar el análisis.   \n",
    "\n",
    "\n",
    "La normalización mejora la consistencia y eficiencia del procesamiento de texto porque:\n",
    "\n",
    "1. Permite que los sistemas informáticos interpreten el lenguaje humano con mayor facilidad, asegurando que iguales palabras sean reconocidas correctamente.​\n",
    "2. Es fundamental para tareas como la búsqueda eficiente, la extracción de información, la clasificación textual y la traducción automática.​\n",
    "3. Reduce las ambigüedades que puede tener el texto, facilitando el análisis estadístico, sintáctico y semántico.\n",
    "\n",
    "En definitiva, la **normalización** es uno de los primeros pasos en cualquier proyecto de NLP y constituye la base sobre la que se desarrollan algoritmos capaces de entender el lenguaje humano en aplicaciones como chatbots, traductores automáticos y sistemas de análisis de sentimiento.   \n",
    "\n",
    "\n",
    "En este caso, empezamos con el *tokenizador*, que reconoce formas como *caminar, Caminar y CAMINAR* como formas distintas. Además, el documento puede tener números y palabras compuestas por caracteres alfanuméricos y otros símbolos tales como #Ar1anaG. Si no nos interesan estas palabras, y queremos que en la lista aparezcan solamente las formas convencionales (por ejemplo, caminar, sólo en minúsculas) debemos normalizar nuestro texto.    \n",
    "\n",
    "Aprovecharemos también para descartar palabras muy cortas (menores a 4 caracteres) para filtrar aún más nuestros tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [t.lower() for t in lexical_tokens if len(t) > 3 and t.isalpha()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si lo agrupamos todo en una única función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['texto', 'prueba', 'tokens', 'quedarán', 'normalización']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "def normalize(text):\n",
    "    doc = nlp(text)\n",
    "    words = [t.orth_ for t in doc if not t.is_punct | t.is_stop]\n",
    "    lexical_tokens = [t.lower() for t in words if len(t) > 3 and     \n",
    "    t.isalpha()]\n",
    "    return lexical_tokens\n",
    "\n",
    "word_list = normalize(\"Soy un texto de prueba. ¿Cuántos tokens me quedarán después de la normalización?\")\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisando todos los pasos de un vistazo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de la normalización:\n",
      "Texto original:\n",
      "Soy un texto de prueba. ¿Cuántos tokens me quedarán después de la normalización?\n",
      "Palabras filtradas:\n",
      "['texto', 'prueba', 'tokens', 'quedarán', 'normalización']\n",
      "Tokens:\n",
      "['Soy', 'un', 'texto', 'de', 'prueba', '.', '¿', 'Cuántos', 'tokens', 'me', 'quedarán', 'después', 'de', 'la', 'normalización', '?']\n",
      "Tokens léxicos (sin puntuación ni stopwords):\n",
      "['texto', 'prueba', 'tokens', 'quedarán', 'normalización']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Soy un texto de prueba. ¿Cuántos tokens me quedarán después de la normalización?\"\"\"\n",
    "doc = nlp(text)\n",
    "tokens = [t.orth_ for t in doc]\n",
    "lexical_tokens = [t.orth_ for t in doc if not t.is_punct | t.is_stop]   \n",
    "words = [t.lower() for t in lexical_tokens if len(t) > 3 and t.isalpha()]\n",
    "print(\"Resultados de la normalización:\")\n",
    "print(\"Texto original:\")\n",
    "print(text)\n",
    "print(\"Palabras filtradas:\")\n",
    "print(words)\n",
    "print(\"Tokens:\")\n",
    "print(tokens)\n",
    "print(\"Tokens léxicos (sin puntuación ni stopwords):\")\n",
    "print(lexical_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEMATIZACIÓN\n",
    "\n",
    "En español, por ejemplo, sabemos que canto, cantas, canta, cantamos, cantáis, cantan son distintas formas (conjugaciones) de un mismo verbo (*cantar*). Y que niña, niño, niñita, niños, niñotes, y otras más, son distintas formas del vocablo *niño*. Así que sería genial poder obviar las diferencias y juntar todas estas variantes en un mismo término.    \n",
    "   \n",
    "   \n",
    "Y eso es precisamente lo que hace la **lematización**: relaciona una palabra flexionada o derivada con su forma canónica o lema. Y un lema no es otra cosa que la forma que tienen las palabras cuando las buscas en el diccionario. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ser', 'uno', 'texto', 'que', 'pedir', 'a', 'grito', 'que', 'él', 'procesen', '.', 'por', 'ese', 'yo', 'cantar', ',', 'tú', 'canta', ',', 'él', 'cantar', ',', 'yo', 'cantar', ',', 'cantáis', ',', 'cantar', '…']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "text = \"\"\"Soy un texto que pide a gritos que lo procesen. Por eso yo canto, tú cantas, ella canta, nosotros cantamos, cantáis, cantan…\"\"\"\n",
    "doc = nlp(text)\n",
    "lemmas = [tok.lemma_.lower() for tok in doc]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como el proceso de lematización toma en consideración la probable clase de palabra (adjetivo, verbo, sustantivo…) — también llamados POS — es posible usar dicha información para filtrar la lista de lemas.   \n",
    "La siguiente línea excluye los pronombres de nuestra lista de lemas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ser', 'uno', 'texto', 'pedir', 'a', 'grito', 'procesen', '.', 'por', 'cantar', ',', 'canta', ',', 'cantar', ',', 'cantar', ',', 'cantáis', ',', 'cantar', '…']\n"
     ]
    }
   ],
   "source": [
    "lemmas_no_pron = [tok.lemma_.lower() for tok in doc if tok.pos_ != 'PRON']\n",
    "print(lemmas_no_pron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la línea siguiente, descartamos los verbos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ser', 'uno', 'texto', 'que', 'a', 'grito', 'que', 'él', '.', 'por', 'ese', 'yo', ',', 'tú', 'canta', ',', 'él', ',', 'yo', ',', 'cantáis', ',', '…']\n"
     ]
    }
   ],
   "source": [
    "lemmas_no_verb = [tok.lemma_.lower() for tok in doc if tok.pos_ != 'VERB']\n",
    "print(lemmas_no_verb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora, combinamos ambas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ser', 'uno', 'texto', 'a', 'grito', '.', 'por', ',', 'canta', ',', ',', ',', 'cantáis', ',', '…']\n"
     ]
    }
   ],
   "source": [
    "lemas_no_pron_no_verb = [tok.lemma_.lower() for tok in doc if tok.pos_ != 'PRON' and tok.pos_ != 'VERB']\n",
    "print(lemas_no_pron_no_verb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El motivo por el que en la lista siguen apareciendo algunas formas verbales como \"canta\" y \"cantáis\" a pesar de la condición impuesta (tok.pos_ != 'VERB') es porque spaCy clasifica esos tokens con una etiqueta PosTag diferente, específicamente como auxiliares ('AUX') y no como verbos ('VERB').\n",
    "\n",
    "En español y otros idiomas, spaCy distingue entre verbos principales ('VERB') y verbos auxiliares ('AUX'). Por ejemplo, en el texto, algunas formas verbales son marcadas como auxiliares o tienen etiquetas específicas que no coinciden con 'VERB'. Por eso, el filtro que excluye solo los tokens con pos_ == 'VERB' no elimina los tokens con etiqueta AUX.   \n",
    "\n",
    "En el caso específico de \"cantáis\", spaCy lo 'cataloga' como 'PROPN', nombre propio y, para \"canta\", en esta ocasión se clasifica como \"NOUN\", dado que el contexto no es suficiente o puede ser ambiguo. Esto sucede porque el modelo *es_core_news_sm* es limitado y en ocasiones asigna etiquetas POS incorrectas a ciertas palabras, especialmente en formas verbales conjugadas que tienen pocas apariciones o ambigüedad en los datos de entrenamiento\n",
    "\n",
    "Para ello es necesario ampliar la condición y eliminar también las formas auxiliares, de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uno', 'texto', 'a', 'grito', '.', 'por', ',', 'canta', ',', ',', ',', ',', '…']\n"
     ]
    }
   ],
   "source": [
    "#lemas_no_pron_no_verb_no_aux = [tok.lemma_.lower() for tok in doc if tok.pos_ != 'PRON' and tok.pos_ != 'VERB' and tok.pos_ != 'AUX' and tok.pos_ != 'PROPN']\n",
    "lemas_no_pron_no_verb_no_aux = [tok.lemma_.lower() for tok in doc if tok.pos_ not in ['PRON', 'VERB', 'AUX', 'PROPN']]\n",
    "\n",
    "print(lemas_no_pron_no_verb_no_aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, echemos un vistazo a cúal es la clasificación POS (Part-Of-Speech) que realiza spaCy con el modelo actual sobre cada uno de los lemas de nuestro texto de ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clasificación POS de spaCy para cada token:\n",
      "Soy  -  ser  -  AUX\n",
      "un  -  uno  -  DET\n",
      "texto  -  texto  -  NOUN\n",
      "que  -  que  -  PRON\n",
      "pide  -  pedir  -  VERB\n",
      "a  -  a  -  ADP\n",
      "gritos  -  grito  -  NOUN\n",
      "que  -  que  -  PRON\n",
      "lo  -  él  -  PRON\n",
      "procesen  -  procesen  -  VERB\n",
      ".  -  .  -  PUNCT\n",
      "Por  -  por  -  ADP\n",
      "eso  -  ese  -  PRON\n",
      "yo  -  yo  -  PRON\n",
      "canto  -  cantar  -  VERB\n",
      ",  -  ,  -  PUNCT\n",
      "tú  -  tú  -  PRON\n",
      "cantas  -  canta  -  NOUN\n",
      ",  -  ,  -  PUNCT\n",
      "ella  -  él  -  PRON\n",
      "canta  -  cantar  -  VERB\n",
      ",  -  ,  -  PUNCT\n",
      "nosotros  -  yo  -  PRON\n",
      "cantamos  -  cantar  -  VERB\n",
      ",  -  ,  -  PUNCT\n",
      "cantáis  -  cantáis  -  PROPN\n",
      ",  -  ,  -  PUNCT\n",
      "cantan  -  cantar  -  VERB\n",
      "…  -  …  -  PUNCT\n"
     ]
    }
   ],
   "source": [
    "print(\"Clasificación POS de spaCy para cada token:\")\n",
    "for tok in doc:\n",
    "    print(tok.text,' - ', tok.lemma_,' - ', tok.pos_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ADP - Adposicion - Preposición.   \n",
    "- DET - Determinante.\n",
    "- NOUN - Nombre, sustantivo.\n",
    "- VERB - Verbo, forma verbal.\n",
    "- PUNCT - Signo de puntuación.\n",
    "- PROPN - Nombre propio.\n",
    "- AUX - Auxiliar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La lematización es un proceso clave en muchas tareas prácticas de PLN, pero tiene ***dos costos***:   \n",
    "- Primero, es un proceso que consume recursos (sobre todo tiempo). \n",
    "- Segundo, suele ser probabilística, así que en algunos casos se suelen obtener resultados inesperados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEMMING\n",
    "\n",
    "Es el procedimiento de convertir palabras en raíces. Estas raíces son la parte invariable de palabras relacionadas sobre todo por su forma.    \n",
    "\n",
    "En cierta manera se parece a la lematización, pero los resultados (las raíces) no tienen por qué ser palabras de un idioma.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'pid', 'grit', 'proces', 'cant', 'cant', 'cant', 'cant', 'cant', 'cant']\n"
     ]
    }
   ],
   "source": [
    "# Stemming con NLTK (instalar NLTK si no lo tienes: pip install nltk)\n",
    "import nltk\n",
    "from nltk import SnowballStemmer\n",
    "spanishstemmer=SnowballStemmer('spanish')\n",
    "text = \"\"\"Soy un texto que pide a gritos que lo procesen. Por eso yo canto, tú cantas, ella canta, nosotros cantamos, cantáis, cantan…\"\"\"\n",
    "tokens = normalize(text) # crear una lista de tokens\n",
    "stems = [spanishstemmer.stem(token) for token in tokens]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar, para encontrar las raíces en español nos hemos valido de otra librería de Python llamada **nltk**. Es otra librería fundamental para el procesamiento de lenguaje natural.    \n",
    "\n",
    "En **nltk** hay muchas funciones para tareas de este tipo, en varios idiomas. En el ejemplo se ha utilizado el ***Snowball Stemmer*** porque funciona no sólo en inglés, sino en otras lenguas como el español."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El *stemming* es mucho más rápido desde el punto de vista del procesamiento que la *lematización*. También tiene como ventaja que *reconoce relaciones entre palabras de distinta clase*. Podría reconocer, por ejemplo, que picante y picar tienen como raíz pic-. En otras palabras, el stemming puede reducir el número de elementos que forman nuestros textos. Y eso, en muchos casos, es lo se suele tener como objetivo.\n",
    "\n",
    "Por otra parte, una desventaja del stemming es que sus algoritmos son más simples que los de lematización. Pueden “recortar” demasiado la raíz y encontrar relaciones entre palabras que realmente no existen (overstemming).    \n",
    "También puede suceder que deje raíces demasiado extensas o específicas, y que tengamos más bien un déficit de raíces (understemming), en cuyo caso palabras que deberían convertirse en una misma raíz no lo hacen. No hay mucho que hacer con eso, pero el stemming es una muy buena solución de compromiso en la mayoría de los casos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión   \n",
    "\n",
    "La **tokenización**, **normalización**, **lematización** y **“radicalización”(stemming)** de un texto suelen ser procedimientos fundamentales para muchas tareas relacionadas con la extracción automática de características y de datos de los textos.    \n",
    "Estos procedimientos están en la base de los grandes y pequeños buscadores de información.    \n",
    "\n",
    "El **stemming** suele ser una buena solución cuando no importa demasiado la precisión y se requiere de un procesamiento eficiente.    \n",
    "La **lematización** suele funcionar mejor cuando se necesita procesar palabras de manera similar a como lo hace un ser humano."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
