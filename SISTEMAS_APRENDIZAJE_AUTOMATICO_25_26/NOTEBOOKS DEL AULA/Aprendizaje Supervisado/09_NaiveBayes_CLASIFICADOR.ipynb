{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f97f7b6a9be3f7b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ALGORITMO NAIVE-BAYES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645660e9",
   "metadata": {},
   "source": [
    "## Funcionamiento del algoritmo Naive-Bayes   ([V√≠deo explicativo](https://youtu.be/vXyT-q3ZNzA))   \n",
    "\n",
    " \n",
    "Naive Bayes es un modelo bayesiano, es decir, probabil√≠stico. Esto quiere decir que, las predicciones se basan en calcular probabilidades.\n",
    "\n",
    "Adem√°s, tal como su nombre indica, es un modelo Naive, es decir, ing√©nuo. Esto es porque Naive Bayes asume que el efecto de una variable es independiente al resto de variables. De esta forma, el c√°lculo de las probabilidades es mucho m√°s sencillo.\n",
    "\n",
    "En definitiva, si en un mensaje aparece la palabra ¬´buenos¬ª esto ser√° independiente de que aparezce la palabra ¬´d√≠as¬ª. Como puedes esperar, en la realidad esto no es as√≠. Sin embargo, esta asunci√≥n facilita mucho los c√°lculos, tal como veremos m√°s adelantes.\n",
    "\n",
    "Y es que, en t√©rminos matem√°ticos, el c√°lculo de Naive Bayes se traduce en la siguiente f√≥rmula, la cual se conoce como regla de Bayes:   \n",
    "\n",
    "$$\n",
    "P(A|B) =\\frac {P(B|A)P(A)}{P(B)}   \n",
    "$$   \n",
    "\n",
    "d√≥nde:   \n",
    "\n",
    "- P(A|B): probabilidad de que ocurra A sabiendo que B ya ha ocurrido.\n",
    "- P(B|A): probabilidad de que ocurra B sabiendo que se ha dado A.\n",
    "- P(A): probabilidad de que ocurra A.\n",
    "- P(B): probabilidad de que ocurra B.   \n",
    "\n",
    "Teniendo esto en cuenta, depende del tipo de distribuci√≥n que usemos para calcular las probabilidades, tendremos un tipo u otro de implementaci√≥n de Naive Bayes.\n",
    "\n",
    "En general hay dos principales implementaciones de Naive Bayes:    \n",
    "\n",
    "- Naive Bayes Gaussiana    \n",
    "- Naive Bayes Multinominal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1659ca73",
   "metadata": {},
   "source": [
    "### Naive Bayes Gaussiano   \n",
    "\n",
    "Como su nombre indica, Naive Bayes Gaussiano asume que los datos siguen una distribuci√≥n Gaussiana. En este caso, si desarrollamos la f√≥rmula que hemos visto anteriormente obtendremos la siguiente funci√≥n:\n",
    "$$\n",
    "\n",
    "(P(x_i|y)= \\frac{1}{\\sqrt{2\\pi\\sigma^{2}{y}}}e^{\\frac{-(x-\\mu{y})^2}{2\\sigma^{2}_{y}}})\n",
    "\n",
    "$$   \n",
    "\n",
    "Donde:\n",
    "\n",
    "En el fondo, lo que Naive Bayes Gaussiano hace es suponer una distribuci√≥n normal para cada variable y clase dado el promedio y desviaci√≥n t√≠pica de los datos obtenidos. De esta forma, si se desea realizar una clasificaci√≥n binaria, para cada variable tendremos dos distribuciones (una por clase), tal como se muestra en la siguiente imagen:   \n",
    "<div align=\"center\">\n",
    "    <img src=\"./img/nbgauss1.webp\" width=\"800\">   \n",
    "</div>   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81706b3",
   "metadata": {},
   "source": [
    "As√≠ pues, cuando nos llegue una observaci√≥n calcularemos la probabilidad de que ese valor provenga de cada una de las distribuciones, tal como se muestra en la siguiente imagen:   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3725af",
   "metadata": {},
   "source": [
    "   \n",
    "<div align=\"center\">\n",
    "    <img src=\"./img/nbgauss2.webp\" width=\"800\">   \n",
    "</div>   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da18a21e",
   "metadata": {},
   "source": [
    "\n",
    "Se repite este proceso para cada una de las variables y se obtiene la probabilidad final de que esa observaci√≥n pertenezca a cada grupo. Por √∫ltimo, **aquella clase que obtenga mayor probabilidad ser√° la predicci√≥n del algoritmo**.\n",
    "\n",
    "**NOTA:** A la hora de obtener las probabilidades se suelen tomar logaritmos. De esta forma, se evitan problemas en el c√°lculo de la probabilidad total cuando la probabilidad en una de las variables es muy baja."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94603e7",
   "metadata": {},
   "source": [
    "### Naive Bayes Multinomial   \n",
    "\n",
    "Naive Bayes Multinomial se basa en asumir una **distribuci√≥n Multinomial**. La distribuci√≥n Multinomial es una extensi√≥n de la distribuci√≥n Binomial, de tal forma que la probabilidad de cada resultado es independiente y su suma siempre ser√° la unidad.\n",
    "\n",
    "As√≠ pues, en el caso de Naive Bayes Multinomial deberemos:\n",
    "\n",
    "1. Calcular la probabilidad de que se de cada una de las clases.\n",
    "2. Probabilidad de que cada valor se de dentro de una misma clase.\n",
    "3. Para cada clase posible, calcular la probabilidad final de que, dados los datos de entrada, esos datos pertenezcan a esa clase.    \n",
    "\n",
    "A continuaci√≥n se muestra un ejemplo te√≥rico:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f529dfd",
   "metadata": {},
   "source": [
    "### Ejemplo funcionamiento N-B Multinomial   \n",
    "\n",
    "Se desea clasificar mensajes de texto entre **spam** o **no spam**. Para ello se dispone de 20 mensajes diferentes.\n",
    "\n",
    "Lo primero que se va a realizar es crear una tabla que indique *cu√°ntas veces ha aparecido cada palabra en los casos en los que el mensaje era Spam* y *cu√°ntas veces ha aparecido en los mensajes no Spam*. Supongamos que la tabla es la siguiente:   \n",
    "```\n",
    "| Tipo de Documento | Estimado | Amigo | Comida | Dinero |\n",
    "|-------------------|----------|-------|--------|--------|\n",
    "| No Spam           | 8        | 5     | 3      | 1      |\n",
    "| Spam              | 2        | 1     | 0      | 4      |\n",
    "```\n",
    "Partiendo de esta tabla es posible calcular c√≥mo de probable es que aparezca la palabra ¬´Estimado¬ª dentro de un mensaje ¬´No Spam¬ª. Es decir, conocer la proporci√≥n de veces que la palabra ¬´Estimado¬ª ha aparecido en los mensajes ¬´No Spam¬ª:\n",
    "$$\n",
    "P(Estimado|No Spam) = \\frac {8}{8+5+3+1} = 0.47\n",
    "$$\n",
    "Si se aplica este mismo proceso para cada una de las palabras y cada una de las clases, se obtiene la siguiente tabla:   \n",
    "```\n",
    "| Tipo de Documento | Estimado | Amigo | Comida | Dinero |\n",
    "|-------------------|----------|-------|--------|--------|\n",
    "| No Spam           | 0.47     | 0.29  | 0.18   | 0.06   |\n",
    "| Spam              | 0.29     | 0.14  | 0      | 0.57   |\n",
    "```   \n",
    "Por otro lado, es necesario tambi√©n conocer la probabilidad de que una palabra sea Spam o no sea Spam, esto es, la proporci√≥n de palabras Spam y no Spam.\n",
    "$$\n",
    "Prob(No Spam) = \\frac{8 + 5 + 3 + 1}{8 + 5 + 3 + 1 + 2 + 1 + 0 + 4} = \\frac{17}{17+7} = 0.71   \n",
    "$$\n",
    "$$\n",
    "Prob(Spam) = \\frac{2 + 1 + 0 + 4}{8 + 5 + 3 + 1 + 2 + 1 + 0 + 4} = \\frac{7}{17+7} = 0.29\n",
    "$$\n",
    "Con esta informaci√≥n, suponiendo que llega un mensaje con las palabras ¬´Estimado Amigo¬ª. Ahora s√≠, ser√≠a posible aplicar la f√≥rmula anterior para poder clasificar dicho mensaje, tal y como se muestra a continuaci√≥n:\n",
    "$$\n",
    "P(No Spam) \\times P(Estimado | No Spam) \\times P(Amigo | No Spam) = 0.71 \\times 0.47 \\times 0.29 = 0.10   \n",
    "$$\n",
    "$$\n",
    "P(Spam) \\times P(Estimado | Spam) \\times P(Amigo | Spam) = 0.29 \\times 0.29 \\times 0.14 = 0.01\n",
    "$$\n",
    "Como se puede observar, es m√°s probable que ese mensaje sea No Spam que Spam. Por tanto, la predicci√≥n realizada por Naive Bayes es que **ese mensaje no es Spam**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09630fe0",
   "metadata": {},
   "source": [
    "### Probabilidades cero   \n",
    "\n",
    "¬øQu√© sucede si el mensaje que llega tiene las palabras ¬´Dinero¬ª, ¬´Comida¬ª y ¬´Dinero¬ª?   \n",
    "$$\n",
    "P(No Spam) = 0.71 \\times 0.06 \\times 0.18 \\times 0.06 = 0.0004\n",
    "$$\n",
    "$$\n",
    "P(Spam) = 0.29 \\times 0.57 \\times 0.0 \\times 0.57 = 0\n",
    "$$\n",
    "\n",
    "Se observa que, por mucho que intuitivamente el mensaje sea Spam, dado que la palabra ¬´Dinero¬ª aparece mucho en mensajes Spam, el mensaje ser√° clasificado como ¬´No Spam¬ª. Esto es debido a que la palabra ¬´Comida¬ª nunca ha aparecido en un mensaje Spam, por lo que su probabilidad es de cero, haciendo que la probabilidad del mensaje sea de cero.\n",
    "\n",
    "Para solucionar este problema se aplica el **suavizado de Laplace** que consiste, b√°sicamente, en sumar 1 a todas las observaciones. De esta forma, sus probabilidades dejan de ser cero y se consigue evitar este problema.   \n",
    "\n",
    "***Tabla de Frecuencias sin Aplicar Suavizado de Laplace***   \n",
    "```\n",
    "| Tipo de Documento | Estimado | Amigo | Comida | Dinero |\n",
    "|-------------------|----------|-------|--------|--------|\n",
    "| No Spam           | 8        | 5     | 3      | 1      |\n",
    "| Spam              | 2        | 1     | 0      | 4      |\n",
    "```\n",
    "***Tabla de Frecuencias Aplicando Suavizado de Laplace***   \n",
    "```\n",
    "| Tipo de Documento | Estimado | Amigo | Comida | Dinero |\n",
    "|-------------------|----------|-------|--------|--------|\n",
    "| No Spam           | 9        | 6     | 4      | 2      |\n",
    "| Spam              | 3        | 2     | 1      | 5      |\n",
    "```   \n",
    "Se puede observar que en esta ocasi√≥n todas las variables han aparecido al menos una vez, de tal forma que si se calcula las probabilidades no existe ninguna variable con probabilidad igual a cero:   \n",
    "```\n",
    "| Tipo de Documento | Estimado | Amigo | Comida | Dinero |\n",
    "|-------------------|----------|-------|--------|--------|\n",
    "| No Spam           | 0.43     | 0.29  | 0.19   | 0.10   |\n",
    "| Spam              | 0.27     | 0.18  | 0.09   | 0.45   |\n",
    "```   \n",
    "Al volver a clasificar el mensaje con las palabras ¬´Dinero¬ª, ¬´Comida¬ª y ¬´Dinero¬ª la predicci√≥n queda as√≠:\n",
    "$$\n",
    "P(No Spam) = 0.66 \\times 0.10 \\times 0.19 \\times 0.10 = 0.0012\n",
    "$$\n",
    "$$\n",
    "P(Spam) = 0.34 \\times 0.45 \\times 0.09 \\times 0.45 = 0.0062\n",
    "$$\n",
    "Por tanto, al aplicar el suavizado de Laplace el mensaje ha pasado de ser clasificado (incorrectamente) como ¬´No Spam¬ª a ser clasificado (correctamente) como ¬´Spam¬ª.   \n",
    "\n",
    "### Resumen gr√°fico del proceso:   \n",
    "<div align=\"center\">\n",
    "    <img src=\"./img/procesoTF.png\" width=\"900\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae5e1d8",
   "metadata": {},
   "source": [
    "## Ejemplo pr√°ctico con Python: Detecci√≥n de Spam   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8d2799",
   "metadata": {},
   "source": [
    "En este ejemplo se usa el dataset *Spam Collection* obtenido del repositorio web del [UC Irvine ML Repository](https://archive.ics.uci.edu/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8121a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install requests\n",
    "import requests \n",
    "import zipfile\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c59b6815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Localizamos el dataset con la URL de la UCI ML Repository\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
    "data_file = 'SMSSpamCollection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a696251",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OBTENCI√ìN Y DESCOMPRESI√ìN DEL FICHERO ##\n",
    "\n",
    "# Realizamos la petici√≥n para descargar el fichero\n",
    "resp = requests.get(url)\n",
    "\n",
    "# Conseguimos el nombre del fichero\n",
    "filename = url.split('/')[-1]\n",
    "\n",
    "# Descargamos el fichero\n",
    "with open(filename, 'wb') as f:\n",
    "  f.write(resp.content)\n",
    "\n",
    "# Extraemos el contenido del fichero zip\n",
    "with zipfile.ZipFile(filename, 'r') as zip:\n",
    "  zip.extractall('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54e6d6a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "message",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "e13d4e0e-c9ed-4cf8-88f6-71db4c66783c",
       "rows": [
        [
         "0",
         "ham",
         "Ok lar... Joking wif u oni..."
        ],
        [
         "1",
         "spam",
         "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's"
        ],
        [
         "2",
         "ham",
         "U dun say so early hor... U c already then say..."
        ],
        [
         "3",
         "ham",
         "Nah I don't think he goes to usf, he lives around here though"
        ],
        [
         "4",
         "spam",
         "FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, ¬£1.50 to rcv"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                            message\n",
       "0   ham                      Ok lar... Joking wif u oni...\n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "2   ham  U dun say so early hor... U c already then say...\n",
       "3   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "4  spam  FreeMsg Hey there darling it's been 3 week's n..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leemos el dataset\n",
    "data = pd.read_table(data_file, \n",
    "                     header = 0,\n",
    "                     names = ['type', 'message']\n",
    "                     )\n",
    "\n",
    "# Mostramos el encabezado del dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19af2afd",
   "metadata": {},
   "source": [
    "Una vez disponemos del dataset con la informaci√≥n a tratar, procedemos a preparar los datos y aplicar el algoritmo Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb5a466",
   "metadata": {},
   "source": [
    "### Paso 1.- Transformaci√≥n de datos.    \n",
    "\n",
    "En los proyectos de NLP (como este) procesar los datos es muy importante.\n",
    "\n",
    "Se va a seguir los siguientes procesos:\n",
    "\n",
    "- **Tokenizaci√≥n**: consiste en separar los mensajes en palabras para as√≠ poder tratar cada una de las palabras. Esto lo podemos hacer gracias a la funci√≥n `word_tokenize`.\n",
    "- **Eliminaci√≥n de stop words**: suprimir palabras que no aportan valor (preposiciones, conjunciones, etc.). Esto se realiza con el objeto de que no aumente mucho el tama√±o del dataset (ya de por s√≠ grande) y evitar este hecho que no aportar√≠a ning√∫n valor, solo ruido. Existen dos grandes fuentes de stpwords en Python, las de Sklearn y las de la librer√≠a NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddf0cf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jordi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sklearn\n",
    "from sklearn.feature_extraction import text\n",
    "text.ENGLISH_STOP_WORDS\n",
    "# NLTK\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1ee261",
   "metadata": {},
   "source": [
    "\n",
    "- **Stemming o lemmatization**: el objetivo de este paso es que dos palabras que signifiquen lo mismo, pero no est√©n igual escritas, pasen a estar igual escritas. Al fin y al cabo, para el modelo la palabra ¬´bueno¬ª y la palabra ¬´buena¬ª son diferentes. Para hacer esto hay dos grandes t√©cnicas:\n",
    "    - **Stemming**: el stemming consiste en la eliminaci√≥n de las terminaciones de las palabras para quedarnos √∫nicamnete con la ra√≠z. Siguiendo el caso anterior, en ambos casos (bueno, buena) nos quedar√≠amos con ¬´buen¬ª.\n",
    "    - **Lemmatization**: es m√°s utilizado en proyectos de NLP en ingl√©s, ya que en este idioma bueno (good), mejor (better) y el mejor (best) son palabras completamente diferentes. En estos casos el stemming no funcionar√≠a. As√≠ pues, la lematizaci√≥n convertir√≠a todas esas palabras a su basa (good), de tal forma que pasen a significar lo mismo.   \n",
    "\n",
    "Para ejecutar todos estos procesos se dispone del paquete NLTK (Natural Language Toolkit), que es una librer√≠a fundamental para NLP para Python y, en este caso, de mucha utilidas para usar Naive Bayes en Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6a5c9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jordi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jordi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Instalaci√≥n de recursos NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# Tokenization\n",
    "data['tokens'] = data.apply(lambda x: nltk.word_tokenize(x['message']), axis = 1)\n",
    "\n",
    "# Eliminaci√≥n de stopwords\n",
    "data['tokens'] = data['tokens'].apply(lambda x: [item for item in x if item not in stop])\n",
    "\n",
    "# Aplicaci√≥n de stemming (usando el Porter Stemmer, que es uno de los m√°s comunes)\n",
    "stemmer = PorterStemmer()\n",
    "data['tokens'] = data['tokens'].apply(lambda x: [stemmer.stem(item) for item in x])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa3812a",
   "metadata": {},
   "source": [
    "Llegado a este punto, se dispone de los datos ya preprocesados. Ahora es necesario adaptar la informaci√≥n para poder analizarla con el modelo Naive Bayes.   \n",
    "\n",
    "Como se ha visto en la parte te√≥rica, Naive Bayes necesita dos elementos:\n",
    "1. Una **matriz TF** (Term Frequency-Frecuencia de t√©rminos), es decir, una matriz en la que aparece, para cada documento, cu√°ntas veces ha aparecido cada una de las palabras que hay en en todos los documentos.\n",
    "2. Una matriz de apariciones. Es similar a una matriz TF, pero en este caso, en vez de indicar el n√∫mero de apariciones, simplemente indica si esa palabra aparec√≠a o no."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9776d531",
   "metadata": {},
   "source": [
    "El uso de cada uno de ellas depender√° mucho del contextos. En el caso de SMS's, al ser mensajes muy cortos es poco probable que las palabras se repitan, por lo que seguramente ambos enfoques devuelvan el mismo resultado.\n",
    "\n",
    "Sin embargo, en textos m√°s largos resulta m√°s interesante aplicar una matriz TF que una matriz de apariciones.   \n",
    "\n",
    "Por tanto, para conseguir una matriz TF se debe realizar el siguiente proceso:   \n",
    "\n",
    "\n",
    "1. Destokenizar los valores, de tal forma que la columna ¬´tokens¬ª no contenga listas, sino texto. Esto es necesario para que el tercer paso funcione correctamente.\n",
    "2. Hacer un split entre *train* y *test*. Es muy importante realizar el proceso de train y test ***antes de llegar a la matriz TF***. Sino, aparecer√°n problemas de *data leakage*(*\"fuga de datos\": el modelo usa informaci√≥n durante el entrenamiento que no estar√≠a disponible en el momento de hacer predicciones en producci√≥n. Esto hace que el modelo parezca muy bueno en validaci√≥n, pero falle en la vida real.*) y puede afectar al resultado (incluso si se permite comprobar que el pipeline de datos es correcto).\n",
    "3. Aplicar la funci√≥n `CountVectorizer` del m√≥dulo `feature_extraction.text` de *Sklearn* a los datos de train y test. Esta funci√≥n permite crear la matriz TF o, si se indica el par√°metro `binary = True`, se genera una matriz de apariciones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f655cf",
   "metadata": {},
   "source": [
    "A continuaci√≥n se muestra c√≥mo poder aplicar la matriz TF para poder entrenar el modelo de NLP con Naive Bayes en Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e64eb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Unificamos las cadenas de texto de nuevo\n",
    "data['tokens'] = data['tokens'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c02a7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset en train y test\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    data['tokens'], \n",
    "    data['type'], \n",
    "    test_size= 0.2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86542cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creaci√≥n del vectorizador\n",
    "vectorizer = CountVectorizer(\n",
    "    strip_accents = 'ascii', \n",
    "    lowercase = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c41fe010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento del vectorizador y transformaci√≥n de los datos\n",
    "vectorizer_fit = vectorizer.fit(x_train)\n",
    "x_train_transformed = vectorizer_fit.transform(x_train)\n",
    "x_test_transformed = vectorizer_fit.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1113b861",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo Naive Bayes Multinomial (**MultinomialNB**)   \n",
    "\n",
    "Una vez se dispone de los datos transformados, queda entrenar el modelo. Para proyectos de texto, la funci√≥n que mejor se adapta es la funci√≥n `MultinomialNB` de Sklearn.\n",
    "\n",
    "Queda ejecutar el fit y el predict tanto en train como en test.    \n",
    "Adem√°s, para visualizar la capacidad predictiva del modelo se visualizar√° tanto el `balanced accuracy` como la `matriz de confusi√≥n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c59902e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construcci√≥n del modelo\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Entrenamiento del clasificador\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes_fit = naive_bayes.fit(x_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc4c7912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones\n",
    "train_predict = naive_bayes_fit.predict(x_train_transformed)\n",
    "test_predict = naive_bayes_fit.predict(x_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc7ef5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score\n",
    "\n",
    "# Funci√≥n para obtener las m√©tricas de evaluaci√≥n\n",
    "def get_scores(y_real, predict):\n",
    "  ba_train = balanced_accuracy_score(y_real, predict)\n",
    "  cm_train = confusion_matrix(y_real, predict)\n",
    "\n",
    "  return ba_train, cm_train \n",
    "\n",
    "# Funci√≥n para imprimir las m√©tricas de evaluaci√≥n\n",
    "def print_scores(scores):\n",
    "  return f\"Balanced Accuracy: {scores[0]}\\nConfussion Matrix:\\n {scores[1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "374f8dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Train Scores\n",
      "Balanced Accuracy: 0.9840119511677905\n",
      "Confussion Matrix:\n",
      " [[3846    8]\n",
      " [  18  584]]\n",
      "\n",
      "\n",
      "## Test Scores\n",
      "Balanced Accuracy: 0.963970849626733\n",
      "Confussion Matrix:\n",
      " [[967   3]\n",
      " [ 10 135]]\n"
     ]
    }
   ],
   "source": [
    "# C√°lculo de las m√©tricas de evaluaci√≥n\n",
    "train_scores = get_scores(y_train, train_predict)\n",
    "test_scores = get_scores(y_test, test_predict)\n",
    "\n",
    "# Impresi√≥n de las m√©tricas\n",
    "print(\"## Train Scores\")\n",
    "print(print_scores(train_scores))\n",
    "print(\"\\n\\n## Test Scores\")\n",
    "print(print_scores(test_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97b2a29",
   "metadata": {},
   "source": [
    "Pese a tratarse de un modelo muy b√°sico, tiene una muy buena capacidad predictiva en la clasificaci√≥n de texto.    \n",
    "\n",
    "### Ventajas\n",
    "- Es un modelo muy f√°cilmente interpretable, lo cual es un punto muy positivo sobre todo de cara a NLP, donde existen pocas opciones alternativas que tambi√©n sean interpretables.\n",
    "- Es un modelo muy sencillo de entrenar y de ajustar sus hiperpar√°metros, y a√∫n as√≠ puede dar muy buenos resultados, tal como se ha visto en el ejemplo anterior.\n",
    "- Trabaja muy bien para datasets con muchas variables. Una vez m√°s, esto lo hace muy interesante de cara a NLP.\n",
    "- Sirve tanto para problemas de clasificaci√≥n binaria, como clasificaci√≥n multiclase.   \n",
    "\n",
    "### Inconvenientes\n",
    "- Asumir que las variables son independientes. En la gran mayoria de casos esta suposici√≥n no se cumple.\n",
    "- Aparici√≥n de nuevas palabras o clases. En el caso de proyectos de NLP es muy normal que a la hora de realizar predicciones aparezcan nuevas palabras que el modelo no ha visto a la hora de entrenar. Como resultado, el modelo no tendr√° en cuenta dichas palabras de cara a hacer las predicciones, por lo que ser√° necesario reentrenarlo de forma frecuente.   \n",
    "\n",
    "## Conclusi√≥n\n",
    "\n",
    "Naive Bayes es un modelo muy sencillo de entender, interpretar y aplicar. Por norma general, no suele ser un modelo que funcione muy bien en proyectos de clasificaci√≥n. Sin embargo, cuando se trata de clasificar texto, Naive Bayes es, probablemente, el primero modelo que se deber√≠a probar.\n",
    "\n",
    "En cualquier caso, cuando se trata de un proyecto de clasificaci√≥n de texto, siempre es interesante probar otro tipo de modelos (como los clasificadores SVM) y prestar mucha atenci√≥n al proceso de limpieza y calidad de datos.   \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445ce68d",
   "metadata": {},
   "source": [
    "## Ejemplo con de clasificaci√≥n con el dataset Iris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31630dd1f96b7e22",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Importamos librer√≠as necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T14:28:36.335466300Z",
     "start_time": "2024-02-13T14:28:34.472016300Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b31eeb5cb783d22",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Cargamos el dataset IRIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b93180010cd2ff74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T14:29:37.830692100Z",
     "start_time": "2024-02-13T14:29:37.788668400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e43c4a3d58d653",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Partici√≥n de los conjuntos de TRAIN y TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b70479b26351eea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T14:34:17.667495800Z",
     "start_time": "2024-02-13T14:34:17.656283700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b408bd3df7c9ce",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Generaci√≥n y entrenamiento del modelo (Gaussian Naive-Bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1825bcadc7724831",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T14:35:53.136048800Z",
     "start_time": "2024-02-13T14:35:53.110661200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a76c5ad77404c8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Predicci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35e11539ba777fd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T14:36:48.280000500Z",
     "start_time": "2024-02-13T14:36:48.263672700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb24dbb3fd712dad",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Evaluaci√≥n del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cdf519ffcb7e638",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy del modelo GNB:  0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy del modelo GNB: \", accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa71794",
   "metadata": {},
   "source": [
    "## üéØ EJERCICIO - Clasificaci√≥n con algoritmo GaussianNB sobre dataset LOAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e844de3",
   "metadata": {},
   "source": [
    "Caracter√≠sticas del dataset   \n",
    "\n",
    "\n",
    "| | Variable | Explicaci√≥n |\n",
    "|-|----------|-------------|\n",
    "|0| credit_policy | 1 si el cliente cumple los criterios de concesi√≥n de cr√©dito; 0 en caso contrario. |\n",
    "|1| purpose  |  El prop√≥sito del pr√©stamo. |\n",
    "|2| int_rate |   El tipo de inter√©s del pr√©stamo (a los prestatarios m√°s arriesgados se les asignan tipos de inter√©s m√°s altos). |\n",
    "|3| installment |   Las cuotas mensuales que debe pagar el prestatario si se concede el pr√©stamo. |\n",
    "|4| log_annual_inc |   El logaritmo natural de los ingresos anuales declarados por el prestatario. |\n",
    "|5| dti  |  La relaci√≥n entre la deuda y los ingresos del prestatario (importe de la deuda dividido por los ingresos anuales). |\n",
    "|6| fico |   La puntuaci√≥n crediticia FICO del prestatario.  |\n",
    "|7| days_with_cr_line  |  El n√∫mero de d√≠as que el prestatario ha tenido una l√≠nea de cr√©dito.  |\n",
    "|8| revol_bal  |  El saldo renovable del prestatario (importe pendiente de pago al final del ciclo de facturaci√≥n de la tarjeta de cr√©dito). |\n",
    "|9| revol_util |   La tasa de utilizaci√≥n de la l√≠nea renovable del prestatario (el importe de la l√≠nea de cr√©dito utilizada en relaci√≥n con el cr√©dito total disponible). |\n",
    "|10| inq_last_6mths |   El n√∫mero de consultas realizadas por los acreedores al prestatario en los √∫ltimos 6 meses. |\n",
    "|11| delinq_2yrs  |  El n√∫mero de veces que el prestatario ha tenido un retraso de m√°s de 30 d√≠as en un pago en los √∫ltimos 2 a√±os.  |\n",
    "|12| pub_rec  |  El n√∫mero de registros p√∫blicos desfavorables del prestatario.  |\n",
    "|13| not_fully_paid  |  1 si el pr√©stamo no est√° totalmente pagado; 0 en caso contrario.  |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4563de91",
   "metadata": {},
   "source": [
    "Se debe presentar un clasificador para predecir si un pr√©stamo se devolver√° bas√°ndose en los datos anteriores.    \n",
    "Es necesario analizar tres modelos de algoritmos de clasificaci√≥n y optar por un √∫nico finalista.   \n",
    "\n",
    "Ademas, hay dos cosas que se deben tener en cuenta:   \n",
    "- En primer lugar, hay un desequilibrio de clases: hay menos ejemplos de pr√©stamos que no se han pagado en su totalidad. \n",
    "- En segundo lugar, es m√°s importante predecir con precisi√≥n si un pr√©stamo no se devolver√° que si se devolver√°. (Se debe justificar como se ha tenido en cuenta esto en el entrenamiento y la evaluaci√≥n del/de los modelo/s y razonar por qu√© se ha elegido como definitivo el modelo que se seleccione como modelo a implementar en una supuesta producci√≥n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d881702",
   "metadata": {},
   "source": [
    "### ¬øNo sabes por d√≥nde empezar?\n",
    "\n",
    "\n",
    "üó∫Ô∏è **Explorar**: genera una matriz de correlaci√≥n entre las columnas num√©ricas.    \n",
    "- ¬øQu√© columnas est√°n correlacionadas positiva y negativamente entre s√≠? \n",
    "- ¬øCambia si lo segmentas por el prop√≥sito del pr√©stamo?   \n",
    "\n",
    "üìä **Visualizar**: Grafica histogramas para cada columna num√©rica con un elemento de color para segmentar las barras por `not_fully_paid`.   \n",
    "üîé **Analizar**: ¬øLos pr√©stamos con el mismo prop√≥sito tienen cualidades similares que no comparten los pr√©stamos con prop√≥sitos diferentes? Se puede considerar solo los pr√©stamos pagados en su totalidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a63f2893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librer√≠as necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "651df4f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "credit.policy",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "purpose",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "int.rate",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "installment",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "log.annual.inc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "dti",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "fico",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "days.with.cr.line",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "revol.bal",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "revol.util",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "inq.last.6mths",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "delinq.2yrs",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "pub.rec",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "not.fully.paid",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "714877bd-9bfd-4787-909e-5589aac8921c",
       "rows": [
        [
         "0",
         "0",
         "1",
         "debt_consolidation",
         "0.1189",
         "829.1",
         "11.35040654",
         "19.48",
         "737",
         "5639.958333",
         "28854",
         "52.1",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "1",
         "1",
         "1",
         "credit_card",
         "0.1071",
         "228.22",
         "11.08214255",
         "14.29",
         "707",
         "2760.0",
         "33623",
         "76.7",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "2",
         "2",
         "1",
         "debt_consolidation",
         "0.1357",
         "366.86",
         "10.37349118",
         "11.63",
         "682",
         "4710.0",
         "3511",
         "25.6",
         "1",
         "0",
         "0",
         "0"
        ],
        [
         "3",
         "3",
         "1",
         "debt_consolidation",
         "0.1008",
         "162.34",
         "11.35040654",
         "8.1",
         "712",
         "2699.958333",
         "33667",
         "73.2",
         "1",
         "0",
         "0",
         "0"
        ],
        [
         "4",
         "4",
         "1",
         "credit_card",
         "0.1426",
         "102.92",
         "11.29973224",
         "14.97",
         "667",
         "4066.0",
         "4740",
         "39.5",
         "0",
         "1",
         "0",
         "0"
        ]
       ],
       "shape": {
        "columns": 15,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>credit.policy</th>\n",
       "      <th>purpose</th>\n",
       "      <th>int.rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>log.annual.inc</th>\n",
       "      <th>dti</th>\n",
       "      <th>fico</th>\n",
       "      <th>days.with.cr.line</th>\n",
       "      <th>revol.bal</th>\n",
       "      <th>revol.util</th>\n",
       "      <th>inq.last.6mths</th>\n",
       "      <th>delinq.2yrs</th>\n",
       "      <th>pub.rec</th>\n",
       "      <th>not.fully.paid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>0.1189</td>\n",
       "      <td>829.10</td>\n",
       "      <td>11.350407</td>\n",
       "      <td>19.48</td>\n",
       "      <td>737</td>\n",
       "      <td>5639.958333</td>\n",
       "      <td>28854</td>\n",
       "      <td>52.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>0.1071</td>\n",
       "      <td>228.22</td>\n",
       "      <td>11.082143</td>\n",
       "      <td>14.29</td>\n",
       "      <td>707</td>\n",
       "      <td>2760.000000</td>\n",
       "      <td>33623</td>\n",
       "      <td>76.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>0.1357</td>\n",
       "      <td>366.86</td>\n",
       "      <td>10.373491</td>\n",
       "      <td>11.63</td>\n",
       "      <td>682</td>\n",
       "      <td>4710.000000</td>\n",
       "      <td>3511</td>\n",
       "      <td>25.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>0.1008</td>\n",
       "      <td>162.34</td>\n",
       "      <td>11.350407</td>\n",
       "      <td>8.10</td>\n",
       "      <td>712</td>\n",
       "      <td>2699.958333</td>\n",
       "      <td>33667</td>\n",
       "      <td>73.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>0.1426</td>\n",
       "      <td>102.92</td>\n",
       "      <td>11.299732</td>\n",
       "      <td>14.97</td>\n",
       "      <td>667</td>\n",
       "      <td>4066.000000</td>\n",
       "      <td>4740</td>\n",
       "      <td>39.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  credit.policy             purpose  int.rate  installment  \\\n",
       "0      0              1  debt_consolidation    0.1189       829.10   \n",
       "1      1              1         credit_card    0.1071       228.22   \n",
       "2      2              1  debt_consolidation    0.1357       366.86   \n",
       "3      3              1  debt_consolidation    0.1008       162.34   \n",
       "4      4              1         credit_card    0.1426       102.92   \n",
       "\n",
       "   log.annual.inc    dti  fico  days.with.cr.line  revol.bal  revol.util  \\\n",
       "0       11.350407  19.48   737        5639.958333      28854        52.1   \n",
       "1       11.082143  14.29   707        2760.000000      33623        76.7   \n",
       "2       10.373491  11.63   682        4710.000000       3511        25.6   \n",
       "3       11.350407   8.10   712        2699.958333      33667        73.2   \n",
       "4       11.299732  14.97   667        4066.000000       4740        39.5   \n",
       "\n",
       "   inq.last.6mths  delinq.2yrs  pub.rec  not.fully.paid  \n",
       "0               0            0        0               0  \n",
       "1               0            0        0               0  \n",
       "2               1            0        0               0  \n",
       "3               1            0        0               0  \n",
       "4               0            1        0               0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargamos el dataset loan.csv\n",
    "data = pd.read_csv('./datasets/loan.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d0ee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu c√≥digo aqu√≠ #"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
